{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPyImage\n",
    "from tqdm import tqdm\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "        \n",
    "    @staticmethod\n",
    "    def scaled_dot_product(q, k, v, mask=None):\n",
    "        _k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "    \n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Hidden dimensionality of the input\n",
    "            model_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout - Dropout to apply inside the model\n",
    "            input_dropout - Dropout to apply on the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout),\n",
    "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
    "                                              input_dim=self.hparams.model_dim,\n",
    "                                              dim_feedforward=2*self.hparams.model_dim,\n",
    "                                              num_heads=self.hparams.num_heads,\n",
    "                                              dropout=self.hparams.dropout)\n",
    "        # Output classifier per sequence lement\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim, filter_dim,num_heads,drop_rate):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.multi_head=MultiHeadAttention(embed_dim,drop_rate,num_heads)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        # feed forward network\n",
    "        self.layer_norm_ffn=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn=FFN(embed_dim,filter_dim,drop_rate)\n",
    "        self.drop_ffn=nn.Dropout(drop_rate)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        y=self.layer_norm(x)\n",
    "        y=self.multi_head(y,y,y,mask)\n",
    "        y=self.drop(y)\n",
    "        y=y+x\n",
    "        \n",
    "        #ffn\n",
    "        \n",
    "        z=self.layer_norm_ffn(y)\n",
    "        z=self.ffn(z)\n",
    "        z=self.drop_ffn(z)\n",
    "        z=z+y\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,filter_dim,num_heads,drop_rate):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        self.multi_head=MultiHeadAttention(embed_dim,drop_rate,num_heads)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.enc_dec_layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        self.enc_dec_multi_head=MultiHeadAttention(embed_dim,drop_rate,num_heads)\n",
    "        self.enc_dec_drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.layer_norm_ffn=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn=FFN(embed_dim,filter_dim,drop_rate)\n",
    "        self.drop_ffn=nn.Dropout(drop_rate)\n",
    "        \n",
    "        def forward(self,x,encoded, mask_1,mask_2):\n",
    "            \n",
    "            y=self.layer_norm(x)\n",
    "            y=self.multi_head(y,y,y,mask_1)\n",
    "            y=self.drop(y)\n",
    "            \n",
    "            y=x+y\n",
    "            \n",
    "            if encoded is not None:\n",
    "                z=self.enc_dec_layer_norm(y)\n",
    "                z=self.enc_dec_multi_head(y,encoded,encoded,mask_2)\n",
    "                z=self.enc_dec_drop(z)\n",
    "                z=self.enc_dec_drop(z)\n",
    "                \n",
    "                z=z+y\n",
    "            \n",
    "            w=self.layer_norm_ffn(z)\n",
    "            w=self.ffn(w)\n",
    "            w=self.drop_ffn(w)\n",
    "            \n",
    "            w=w+z\n",
    "            \n",
    "            return w\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,filter_dim,drop_rate,n_layers,num_heads):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        encoders=[EncoderLayer(embed_dim,filter_dim,num_heads,drop_rate) for _ in range(n_layers)]\n",
    "        self.encoders=nn.ModuleList(encoders)\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        out=x\n",
    "        for layer in self.encoders:\n",
    "            out=layer(out,mask)\n",
    "        return self.layer_norm(out)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,filter_dim,drop_rate,n_layers,num_heads):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        decoders=[DecoderLayer(embed_dim,filter_dim,num_heads,drop_rate) for _ in range(n_layers)]\n",
    "        self.decoders=nn.ModuleList(decoders)\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        \n",
    "    def forward(self, targets, encoded, mask_1,mask_2):\n",
    "        out = targets\n",
    "        for dec_layer in self.decoders:\n",
    "            out = dec_layer(out, encoded, mask2, mask1)\n",
    "        return self.last_norm(out)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,drop_rate,num_heads=2):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        self.num_heads=num_heads\n",
    "        \n",
    "        self.attention_size=embed_dim//num_heads\n",
    "        self.scale=1/np.sqrt(embed_dim//num_heads)\n",
    "        \n",
    "        self.Q = nn.Linear(embed_dim, num_heads * self.attention_size, bias=False)\n",
    "        self.K = nn.Linear(embed_dim, num_heads * self.attention_size, bias=False)\n",
    "        self.V = nn.Linear(embed_dim, num_heads * self.attention_size, bias=False)\n",
    "        \n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.layer_out=nn.Linear(num_heads * self.attention_size,embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self,Q,K,V, mask):\n",
    "        \n",
    "        batch_size=Q.size(0)\n",
    "        d_K=self.attention_size\n",
    "        d_V=self.attention_size\n",
    "        \n",
    "        q=self.Q(Q)\n",
    "        q=q.view(batch_size,-1,self.num_heads,d_K) # [batch_size,Q_len,heads,d_K]\n",
    "        k=self.K(K)\n",
    "        k=k.view(batch_size,-1,self.num_heads,d_K) # [batch_size,K_len,heads,d_K]\n",
    "        v=self.K(V)\n",
    "        v=v.view(batch_size,-1,self.num_heads,d_V) # [batch_size,K_len,heads,d_K]\n",
    "        \n",
    "        q=q.transpose(1,2) # [batch_size,heads, K_len,d_K]\n",
    "        k=k.transpose(1, 2).transpose(2, 3)\n",
    "        v=v.transpose(1,2) # [batch_size,heads, K_len,d_K]\n",
    "        \n",
    "        out=self.scale*torch.matmul(q,k) # [batch_size,heads,Q_len,K_len]\n",
    "        out=out.masked_fill(mask, float('-inf'))\n",
    "        out=torch.softmax(out,dim=3) #along K_len\n",
    "        out=self.drop(out)\n",
    "\n",
    "        out=torch.matmul(out,v) # [batch_size,heads,Q_len, d_V]\n",
    "        out=out.transpose(1,2).contiguous()\n",
    "        out=out.view(batch_size,-1,self.num_heads*d_V)\n",
    "        \n",
    "        out=self.layer_out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joao/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('Input/yelp_review_polarity_csv/train.csv',header=None)\n",
    "test=pd.read_csv('Input/yelp_review_polarity_csv/test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((560000, 2), (38000, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(columns={0:'label',1:'review'},inplace=True)\n",
    "test.rename(columns={0:'label',1:'review'},inplace=True)\n",
    "\n",
    "train['label']=(train['label']==2).astype('int')\n",
    "test['label']=(test['label']==2).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    280000\n",
       "1    280000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('label')['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.sample(50000,replace=False)\n",
    "test=test.sample(20000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text_=re.sub('[^a-z\\d]',' ',text.lower())\n",
    "    tokens=text_.split()\n",
    "    tokens=[word for word in tokens if word not in stopwords.words('english') and len(word)>1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    result=pool.map(clean_text, [rev for rev in train.review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=Counter()\n",
    "for cntr in result:\n",
    "    vocabulary.update(cntr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_5k=vocabulary.most_common(5000)\n",
    "vocab_5k={word[0]: i+1 for i, word in enumerate(vocab_5k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_seq(lst):\n",
    "    seq=[]\n",
    "    for token in lst:\n",
    "        if token in vocab_5k:\n",
    "            seq.append(vocab_5k[token])\n",
    "        else:\n",
    "            seq.append(0)\n",
    "            \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    result_seqs=pool.map(word_to_seq, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=nn.Embedding(num_embeddings=5001,embedding_dim=10,padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.LongTensor(result_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[x for x in result if len(x)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # For text generation, input vocab= target vocab\n",
    "    def __init__(self,vocab_size,n_layers,embed_dim,filter_dim,num_heads,dropout_rate):\n",
    "        super(Transformer,self).__init__()\n",
    "            \n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        \n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.encoder=Encoder(embed_dim,filter_dim,dropout_rate,n_layers,num_heads)\n",
    "        self.decoder=nn.Linear(embed_dim,vocab_size)\n",
    "\n",
    "        self.pos=PositionalEncoding(embed_dim,dropout_rate)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        x_embed=self.embedding(x)*math.sqrt(self.embed_dim)\n",
    "        x_pos=self.pos(x_embed)\n",
    "        x_encoded=self.encoder(x_pos,mask)\n",
    "        x_decoded=self.decoder(x_encoded)\n",
    "        \n",
    "        return x_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath='.data/wikitext-2/wiki.test.tokens'\n",
    "valid_filepath='.data/wikitext-2/wiki.valid.tokens'\n",
    "train_filepath= '.data/wikitext-2/wiki.train.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36718lines [00:01, 20940.56lines/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,iter(io.open(train_filepath,encoding=\"utf8\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.stoi) # the size of vocabulary\n",
    "embed_dim = 200 # embedding dimension\n",
    "filter_dim = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_layers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "num_heads = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = Transformer(vocab_size,n_layers, embed_dim,filter_dim,num_heads,dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    \n",
    "    return ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer, train_data,epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        src_mask=generate_mask(bptt).to(device)\n",
    "        for i in progressbar(range(0, train_data.size(0) - 1, bptt)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "            optimizer.zero_grad()\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = generate_mask(data.size(0)).to(device)\n",
    "            output = model(data.T, src_mask)\n",
    "            loss = criterion(output.view(-1, vocab_size), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print('epoch: ',epoch,', Loss: ',total_loss/train_data.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , Loss:  0.1915034749282067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 , Loss:  0.19086674898931166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:35"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2 , Loss:  0.19033246433494497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3 , Loss:  0.18974748692545867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4 , Loss:  0.18913434315409783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5 , Loss:  0.18857894826618526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6 , Loss:  0.18805452684042895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7 , Loss:  0.18746501500027885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8 , Loss:  0.18699186953741484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9 , Loss:  0.18649346193981958\n"
     ]
    }
   ],
   "source": [
    "train(model,criterion,optimizer,train_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,data_source):\n",
    "    model.eval()\n",
    "    \n",
    "    src_mask=generate_mask(bptt).to(device)\n",
    "    acc=0\n",
    "    n=0\n",
    "    for i in progressbar(range(0, data_source.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = generate_mask(data.size(0)).to(device)\n",
    "        output=model(data.T, src_mask)\n",
    "        output=output.view(-1, vocab_size)\n",
    "        prob=F.softmax(output,dim=-1)\n",
    "        pred=prob.argmax(dim=-1)\n",
    "        acc+=(pred==targets).sum().item()\n",
    "        n+=data.size(0)\n",
    "    \n",
    "    n=n*20\n",
    "    print('accuracy: ',acc/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:16 Time:  0:00:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.050020976019044275\n"
     ]
    }
   ],
   "source": [
    "eval_model(model,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.Tensor([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 2929) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "src_mask=generate_mask(bptt).to(device)\n",
    "acc=0\n",
    "n=0\n",
    "for i in progressbar(range(0, train_data.size(0) - 1, bptt)):\n",
    "    data, targets = get_batch(train_data, i)\n",
    "    if data.size(0) != bptt:\n",
    "        src_mask = generate_mask(data.size(0)).to(device)\n",
    "    output=model(data.T, src_mask)\n",
    "    output=output.view(-1, vocab_size)\n",
    "    prob=F.softmax(output,dim=-1)\n",
    "    pred=prob.argmax(dim=-1)\n",
    "    acc+=(pred==targets).sum().item()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
